{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import bgr8_to_jpeg\n",
    "from jetbot import ObjectDetector\n",
    "from jetbot import Camera\n",
    "from jetbot import Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up camera and robot, load pre-trained SSD model for COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetector('../Notebooks/object_following/ssd_mobilenet_v2_coco.engine')\n",
    "camera = Camera.instance(width=300, height=300)\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories not created becasue they already exist\n"
     ]
    }
   ],
   "source": [
    "# Load COCO labels\n",
    "filename = \"coco_labels.dat\"\n",
    "filehandler = open(filename, 'rb')\n",
    "COCO_labels = pickle.load(filehandler)\n",
    "\n",
    "# Load camera calibration data for undistort\n",
    "filename = \"calibration.dat\"\n",
    "filehandler = open(filename, 'rb')\n",
    "camera_cal = pickle.load(filehandler)\n",
    "mtx = camera_cal['mtx']\n",
    "dist = camera_cal['dist']\n",
    "f_u = mtx[0,0]   # focal center coordinates\n",
    "f_v = mtx[1,1]\n",
    "focal_center = np.array([f_u, f_v])\n",
    "\n",
    "# Mapping between set_motor \"speed\" and measured wheel angular velocity \"omega\"\n",
    "# for 0.1 second motor running time\n",
    "wheel_calibration = {\n",
    "    \"speed\": [0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "    \"omega\": [0.0, 3.85, 9.23, 15.0, 25.8, 29.2, 35.4]\n",
    "}\n",
    "\n",
    "# Open Image Widget\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "BLUE = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (0, 0, 255)\n",
    "\n",
    "diag_dir = 'diagnostics'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(diag_dir)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_bbox_TV = [(102, 90), (161, 134)]\n",
    "\n",
    "pos1_bbox_TV = [(102, 90), (164, 135)]\n",
    "pos1_bbox_stool = [(168, 123), (200, 182)]\n",
    "\n",
    "pos2_bbox_TV = [(99, 79), (170, 128)]\n",
    "pos2_bbox_stool = [(171, 115), (211, 183)]\n",
    "\n",
    "pos2left_bbox_TV = [(136, 64), (218, 132)]\n",
    "pos2left_bbox_stool = [(215, 96), (283, 187)]\n",
    "\n",
    "pos2right_bbox_TV = [(51, 69), (134, 131)]\n",
    "pos2right_bbox_stool = [(131, 122), (164, 184)]\n",
    "\n",
    "pos3_bbox_TV = [(90, 72), (172, 127)]\n",
    "pos3_bbox_stool = [(172, 107), (219, 189)]\n",
    "\n",
    "# Bbox for TV and stool at various camera poses\n",
    "beg_bbox_TV = [(72, 92), (136, 137)]  # Pose (0,0,0)\n",
    "mid_bbox_TV = [(33, 69), (114, 124)]  # Pose (0.5,0,0)\n",
    "\n",
    "end_bbox_stool = [(175, 84), (237, 186)]   # Pose (1.0,0,0)\n",
    "\n",
    "end_bbox_horse = [(147, 103), (247, 201)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(img, mtx, dist, crop=False):\n",
    "    \"\"\"Undistort camera image based on calibration data\"\"\"\n",
    "    h,w = img.shape[:2]\n",
    "    # print (h,w)\n",
    "    newcameramtx, roi=cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))\n",
    "    \n",
    "    # undistort\n",
    "    dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
    "\n",
    "    # crop the image (optional)\n",
    "    if crop:\n",
    "        x,y,w,h = roi\n",
    "        dst = dst[y:y+h, x:x+w]\n",
    "    return dst\n",
    "\n",
    "def draw_bbox(img, width, height, bbox, color, line_width):\n",
    "    bbox_pixel = [(int(width * bbox[0]), int(height * bbox[1])), \n",
    "                  (int(width * bbox[2]), int(height * bbox[3]))]\n",
    "    cv2.rectangle(img, bbox_pixel[0], bbox_pixel[1], color, line_width)\n",
    "    return bbox_pixel\n",
    "\n",
    "def display_detected(img, detections, width, height, debug=False):\n",
    "    \"\"\" put blue bounding boxes on detected objects on image \"\"\"\n",
    "    \n",
    "    for det in detections[0]:\n",
    "        label = COCO_labels[det['label']]\n",
    "        bbox = det['bbox']\n",
    "        bbox_pixel = draw_bbox(img, width, height, bbox, BLUE, 1)\n",
    "        if debug:\n",
    "            print(label,det['label'], bbox_pixel)\n",
    "    return\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "\n",
    "def object_center(object_features):\n",
    "    \"\"\"Computes the center x, y coordinates of the object using its feature points\"\"\"\n",
    "    UL_fp, UR_fp, LR_fp, LL_fp = obj_features\n",
    "    UL_fp_x, UL_fp_y = UL_fp\n",
    "    LR_fp_x, LR_fp_y = LR_fp\n",
    "    center_x = (UL_fp_x + LR_fp_x) / 2.0 - 0.5\n",
    "    center_y = (UL_fp_y + LR_fp_y) / 2.0 - 0.5\n",
    "    return np.array([center_x, center_y])\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections, debug= False):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if debug:\n",
    "            print(center)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n",
    "def create_feature_pts(bbox):\n",
    "    \"\"\" Return 4 feature points from a bounding box \"\"\"\n",
    "    # extract bounding x and y coordinates\n",
    "    UL_fp, LR_fp = bbox\n",
    "    UL_fp_x, UL_fp_y = UL_fp\n",
    "    LR_fp_x, LR_fp_y = LR_fp\n",
    "   \n",
    "    # return UL(upper left), UR(upper right), LR(lower right), LL(lower left) feature points\n",
    "    return [(UL_fp_x, UL_fp_y), (LR_fp_x, UL_fp_y), (LR_fp_x, LR_fp_y), (UL_fp_x, LR_fp_y)]\n",
    "\n",
    "def generate_ref_obj_features(ref_obj, img, detections, width, height, debug=False):\n",
    "    \"\"\" Find reference object in detected objects and generate 4 feature points \"\"\"\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == ref_obj]\n",
    "\n",
    "    # get detection closest to center of field of view and draw it (if model detects multiple\n",
    "    # reference objects)\n",
    "    det = closest_detection(matching_detections)\n",
    "\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        # bound reference object in green\n",
    "        draw_bbox(img, width, height, bbox, GREEN, 1)\n",
    "        # convert to pixel units\n",
    "        bbox_pixel = [(int(width * bbox[0]), int(height * bbox[1])), \n",
    "                       (int(width * bbox[2]), int(height * bbox[3]))]\n",
    "        return create_feature_pts(bbox_pixel) # return 4 feature points of bounding box\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def fp_error(current, goal):\n",
    "    \"\"\" Return error of a feature point \"\"\"\n",
    "    current_x, current_y = current\n",
    "    goal_x, goal_y = goal\n",
    "    # Use Corke's convention of (p*-p) --> gain is a +ve number\n",
    "    return (goal_x-current_x, goal_y-current_y)  \n",
    "\n",
    "def features_error(currentFeatures, goalFeatures):\n",
    "    \"\"\" Return feature points error vector \"\"\"\n",
    "    UL_current, UR_current, LR_current, LL_current = currentFeatures \n",
    "    UL_goal, UR_goal, LR_goal, LL_goal = goalFeatures \n",
    "    \n",
    "    # calculate error for each feature point\n",
    "    UL_error = fp_error(UL_current, UL_goal)\n",
    "    error_vector = np.asarray(UL_error)\n",
    "    UR_error = fp_error(UR_current, UR_goal)\n",
    "    error_vector = np.concatenate((error_vector, UR_error), axis=None)\n",
    "    LR_error = fp_error(LR_current, LR_goal)\n",
    "    error_vector = np.concatenate((error_vector, LR_error), axis=None)\n",
    "    LL_error = fp_error(LL_current, LL_goal)\n",
    "    error_vector = np.concatenate((error_vector, LL_error), axis=None)\n",
    "\n",
    "    # return 1x8 error vector of UL, UR, LR, LL feature points\n",
    "    return np.reshape(error_vector, (1,-1))\n",
    "\n",
    "def image_jacobian(fp, mtx, depth):\n",
    "    \"\"\" Generate image jacobiab L for a feature point \"\"\"\n",
    "    # focal lengths in pixel unit\n",
    "    f_u = mtx[0,0]\n",
    "    f_v = mtx[1,1]\n",
    "    c_u = mtx[0,2]\n",
    "    c_v = mtx[1,2]\n",
    "\n",
    "    # Estimated distance of reference object (m)\n",
    "    Z = depth\n",
    "\n",
    "    # Calculate J of feature point\n",
    "    u_raw, v_raw = fp\n",
    "    u = u_raw - c_u\n",
    "    v = v_raw - c_v    \n",
    "    \n",
    "    L = np.array([[-f_u/Z, 0, u/Z, u*v/f_u, -(f_u+u*u/f_u), v],\n",
    "                  [0, -f_v/Z, v/Z, f_v+v*v/f_v, -u*v/f_v, -u]])\n",
    "    return L\n",
    "\n",
    "def robot2world(robot_orientation):\n",
    "    \"\"\" calculate the robot jacobian \"\"\"\n",
    "    theta = robot_orientation\n",
    "    \n",
    "    return np.array([[math.cos(theta),0],\n",
    "                  [math.sin(theta),0],\n",
    "                  [0,0],\n",
    "                  [0,0],\n",
    "                  [0,0],\n",
    "                  [0,1]])\n",
    "\n",
    "def control2robot(wheel_radius, axle_length):\n",
    "    \"\"\" transform wheel speeds to robot motion in world frame \"\"\"\n",
    "    l = axle_length\n",
    "    r = wheel_radius\n",
    "\n",
    "    return np.array([[r/2, r/2],\n",
    "                  [r/l, -r/l]])\n",
    "\n",
    "def save_snapshot(img, directory, i):\n",
    "    image_path = os.path.join(directory, 'detect'+str(i+1)+'.jpg')\n",
    "    cv2.imwrite(image_path, img) \n",
    "    return\n",
    "\n",
    "def omega2speed(in_val, mapping):\n",
    "    \"\"\" Map wheel angular speed to motor speed setting based on a calibration mapping \"\"\"\n",
    "    \n",
    "    if in_val < 0:\n",
    "        sign = -1\n",
    "        in_val = abs(in_val)\n",
    "    else:\n",
    "        sign = 1\n",
    "        \n",
    "    out_lower = 0\n",
    "    in_lower = 0\n",
    "    out_val = 0\n",
    "\n",
    "    for i, in_upper in enumerate(mapping[\"omega\"]):\n",
    "        # print (i, in_upper)\n",
    "        if in_val < in_upper:\n",
    "            out_upper = mapping[\"speed\"][i]\n",
    "            out_val = out_lower + (in_val - in_lower)/(in_upper - in_lower) \\\n",
    "                *(out_upper-out_lower)\n",
    "            # print(\"yes\", out_val)\n",
    "            break\n",
    "        else:\n",
    "            # print(\"no\")\n",
    "            out_lower = mapping[\"speed\"][i]\n",
    "            in_lower = in_upper\n",
    "            \n",
    "    if out_val is 0:\n",
    "        print (\"Input is too high!!!\")\n",
    "        out_val = 0\n",
    "        \n",
    "    return sign*out_val\n",
    "\n",
    "def rotate_left_2wheels(Rtime):\n",
    "    \n",
    "    robot.set_motors(-0.3, 0.3)\n",
    "    time.sleep(Rtime)\n",
    "    robot.stop()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e101e1668440a9a3888169124a5adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 laptop [(33, 69), (114, 69), (114, 124), (33, 124)]\n",
      "couch 62 [(96, 105), (138, 184)]\n",
      "clock 84 [(259, 114), (291, 182)]\n",
      "clock 84 [(246, 118), (281, 181)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 106), (138, 183)]\n",
      "clock 84 [(259, 113), (290, 183)]\n",
      "clock 84 [(246, 118), (279, 181)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 105), (139, 183)]\n",
      "clock 84 [(258, 113), (291, 182)]\n",
      "clock 84 [(246, 117), (280, 181)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 106), (138, 184)]\n",
      "clock 84 [(259, 113), (290, 182)]\n",
      "clock 84 [(245, 118), (280, 181)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 107), (138, 184)]\n",
      "clock 84 [(258, 114), (290, 182)]\n",
      "clock 84 [(246, 119), (280, 181)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 103), (138, 184)]\n",
      "clock 84 [(259, 112), (290, 182)]\n",
      "clock 84 [(247, 118), (279, 181)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 105), (139, 184)]\n",
      "clock 84 [(258, 113), (289, 182)]\n",
      "clock 84 [(247, 117), (280, 180)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 106), (138, 184)]\n",
      "clock 84 [(258, 114), (290, 182)]\n",
      "clock 84 [(246, 119), (281, 181)]\n",
      "person 1 [(0, 141), (15, 179)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 105), (139, 183)]\n",
      "clock 84 [(256, 109), (291, 182)]\n",
      "clock 84 [(245, 116), (282, 181)]\n",
      "Reference object not detected!!!\n",
      "couch 62 [(96, 105), (138, 183)]\n",
      "clock 84 [(258, 113), (290, 182)]\n",
      "clock 84 [(246, 118), (280, 181)]\n",
      "Reference object not detected!!!\n",
      "FPS: 7.9\n"
     ]
    }
   ],
   "source": [
    "# Display camera image with bounding boxes for detected objects\n",
    "display(widgets.HBox([image_widget]))\n",
    "\n",
    "# robot pose - may need to be updated\n",
    "depth = 2.0\n",
    "orientation = 0 \n",
    "\n",
    "# robot parameters\n",
    "wheel_radius = 0.0325\n",
    "axle_length = 0.12\n",
    "\n",
    "# Control loop parameters\n",
    "gain = 1.4\n",
    "Rtime = 0.1  # The loop run at 8fps --> 0.125s duration\n",
    "num_iter = 10\n",
    "interval = 2\n",
    "\n",
    "no_motion = True  # Flag to disable motor (for debugging)\n",
    "debug = True\n",
    "\n",
    "# Use TV as reference object \n",
    "ref_obj = 72\n",
    "goal_features = create_feature_pts(mid_bbox_TV)\n",
    "print(ref_obj,COCO_labels[ref_obj], goal_features)\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "# Run for fixed # of iterations\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    image = undistort(camera.value, mtx, dist) # undistort camera image\n",
    "    detections = model(image) # Use SSD model to detect objects\n",
    "    display_detected(image, detections, width, height, debug=True) # put bounding boxes on detected objects\n",
    "    \n",
    "    # Detect reference object in camera image and place green bounding box around it\n",
    "    obj_features = generate_ref_obj_features(ref_obj, image, detections, width, height)\n",
    "    \n",
    "    if obj_features is None:\n",
    "        print(\"Reference object not detected!!!\")\n",
    "        continue\n",
    "        \n",
    "    # bound reference object's feature points at goal pose in red\n",
    "    cv2.rectangle(image, goal_features[0], goal_features[2], RED, 2)\n",
    "    error = features_error(obj_features,goal_features)\n",
    "    \n",
    "    if i%interval == 0 and debug:\n",
    "        print(obj_features)\n",
    "        print(error)\n",
    "    \n",
    "    if np.linalg.norm(error) < 5.6:\n",
    "        print (\"Goal point achieved!!!\")\n",
    "        continue\n",
    "        \n",
    "    T = control2robot(wheel_radius, axle_length) # wheel speeds to robot velocities transform\n",
    "    H = robot2world(orientation)  # robot velocities to world frame transform\n",
    "    \n",
    "    # Image Jacobian based on 4 fp of reference object\n",
    "    UL_fp, UR_fp, LR_fp, LL_fp = obj_features\n",
    "    L = np.vstack((image_jacobian(UL_fp, mtx, depth), image_jacobian(UR_fp, mtx, depth), \\\n",
    "              image_jacobian(LR_fp, mtx, depth), image_jacobian(LL_fp, mtx, depth)))\n",
    "    \n",
    "    # implement w = gain * pinv(LHT).error\n",
    "    J = np.dot(L,np.dot(H,T))\n",
    "    inv_J = np.linalg.pinv(J)\n",
    "    wheel_velocities = gain * np.dot(np.linalg.pinv(J), error.T)\n",
    "    robot_velocities = np.dot(T, wheel_velocities)\n",
    "    w_r = omega2speed(wheel_velocities[0,0],wheel_calibration) \n",
    "    w_l = omega2speed(wheel_velocities[1,0],wheel_calibration) \n",
    "    \n",
    "    if i%interval == 0 and debug:\n",
    "        print(\"Iteration \", i)\n",
    "        save_snapshot(image, diag_dir, i)\n",
    "        print(wheel_velocities, (w_l, w_r))\n",
    "        print(robot_velocities)\n",
    "        print(\"Orientation:\", orientation)\n",
    "        print(\"Depth:\", depth)\n",
    "    \n",
    "    if no_motion is False:\n",
    "        robot.set_motors(w_l, w_r)\n",
    "        time.sleep(Rtime)\n",
    "        robot.stop()\n",
    "    \n",
    "    # Update robot's orientation\n",
    "    translation_velocity = robot_velocities[0,0]\n",
    "    angular_velocity = robot_velocities[1,0]\n",
    "    orientation += Rtime*angular_velocity\n",
    "    depth -= Rtime*translation_velocity*math.cos(orientation)\n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(image)  # update image widget with camera image\n",
    "\n",
    "image_widget.value = bgr8_to_jpeg(image)\n",
    "robot.stop()\n",
    "\n",
    "end = time.perf_counter()\n",
    "print (\"FPS: {:.1f}\".format(num_iter/(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf2729b75bd4a9696561360c8103aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 couch [(164, 101), (217, 101), (217, 185), (164, 185)]\n",
      "couch 62 [(157, 118), (196, 187)]\n",
      "[(157, 118), (196, 118), (196, 187), (157, 187)]\n",
      "[[  7 -17  21 -17  21  -2   7  -2]]\n",
      "Iteration  0\n",
      "[[-6.30589661]\n",
      " [-6.9316925 ]] (-0.35728052972744484, -0.34564863584838446)\n",
      "[[-0.21511082]\n",
      " [ 0.16948639]]\n",
      "Orientation: 0\n",
      "Depth: 1.3\n",
      "couch 62 [(159, 116), (196, 188)]\n",
      "clock 84 [(98, 151), (106, 181)]\n",
      "laptop 72 [(27, 59), (107, 118)]\n",
      "66 65 [(11, 187), (295, 293)]\n",
      "couch 62 [(162, 118), (201, 186)]\n",
      "laptop 72 [(28, 63), (115, 120)]\n",
      "clock 84 [(101, 151), (109, 180)]\n",
      "[(162, 118), (201, 118), (201, 186), (162, 186)]\n",
      "[[  2 -17  16 -17  16  -1   2  -1]]\n",
      "Iteration  2\n",
      "[[-4.27738139]\n",
      " [-4.92782308]] (-0.32003388625250906, -0.307943892012473)\n",
      "[[-0.14958457]\n",
      " [ 0.17616129]]\n",
      "Orientation: 0.03317503617108727\n",
      "Depth: 1.3417581131622829\n",
      "laptop 72 [(42, 64), (124, 122)]\n",
      "couch 62 [(171, 118), (211, 184)]\n",
      "couch 62 [(172, 118), (213, 185)]\n",
      "laptop 72 [(47, 66), (126, 122)]\n",
      "[(172, 118), (213, 118), (213, 185), (172, 185)]\n",
      "[[ -8 -17   4 -17   4   0  -8   0]]\n",
      "Iteration  4\n",
      "[[0.58972848]\n",
      " [0.00645587]] (0.2500838424917438, 0.25765881146799074)\n",
      "[[0.009688  ]\n",
      " [0.15796967]]\n",
      "Orientation: 0.06620597795898503\n",
      "Depth: 1.3580169797273174\n",
      "couch 62 [(173, 117), (213, 185)]\n",
      "laptop 72 [(50, 67), (125, 122)]\n",
      "clock 84 [(114, 152), (122, 179)]\n",
      "couch 62 [(177, 117), (221, 185)]\n",
      "laptop 72 [(57, 65), (131, 123)]\n",
      "[(177, 117), (221, 117), (221, 185), (177, 185)]\n",
      "[[-13 -16  -4 -16  -4   0 -13   0]]\n",
      "Iteration  6\n",
      "[[3.55133938]\n",
      " [3.0876329 ]] (0.2900991285671095, 0.29612129068608944)\n",
      "[[0.1078833 ]\n",
      " [0.12558717]]\n",
      "Orientation: 0.09658453439798181\n",
      "Depth: 1.3551877459984771\n",
      "laptop 72 [(63, 66), (135, 126)]\n",
      "couch 62 [(183, 114), (228, 185)]\n",
      "clock 84 [(75, 139), (81, 152)]\n",
      "couch 62 [(189, 111), (236, 187)]\n",
      "laptop 72 [(67, 67), (139, 127)]\n",
      "clock 84 [(79, 140), (83, 153)]\n",
      "clock 84 [(86, 137), (90, 152)]\n",
      "[(189, 111), (236, 111), (236, 187), (189, 187)]\n",
      "[[-25 -10 -19 -10 -19  -2 -25  -2]]\n",
      "Iteration  8\n",
      "[[9.45160959]\n",
      " [9.29535013]] (0.40113258462022033, 0.40384072074784066)\n",
      "[[0.3046381 ]\n",
      " [0.04232027]]\n",
      "Orientation: 0.11663917529598979\n",
      "Depth: 1.3241019063777855\n",
      "couch 62 [(188, 109), (239, 187)]\n",
      "laptop 72 [(67, 65), (139, 125)]\n",
      "66 65 [(162, 200), (299, 298)]\n",
      "couch 62 [(189, 109), (243, 187)]\n",
      "laptop 72 [(62, 60), (140, 123)]\n",
      "clock 84 [(129, 150), (135, 181)]\n",
      "clock 84 [(75, 138), (80, 150)]\n",
      "[(189, 109), (243, 109), (243, 187), (189, 187)]\n",
      "[[-25  -8 -26  -8 -26  -2 -25  -2]]\n",
      "Iteration  10\n",
      "[[10.52164044]\n",
      " [10.47047475]] (0.42149869590405653, 0.4223854495905422)\n",
      "[[0.34112187]\n",
      " [0.01385737]]\n",
      "Orientation: 0.1230009739503769\n",
      "Depth: 1.2626900554155687\n",
      "couch 62 [(190, 106), (249, 188)]\n",
      "laptop 72 [(58, 57), (140, 123)]\n",
      "clock 84 [(120, 152), (126, 181)]\n",
      "clock 84 [(128, 151), (135, 181)]\n",
      "clock 84 [(69, 138), (78, 149)]\n",
      "couch 62 [(192, 101), (257, 187)]\n",
      "laptop 72 [(56, 55), (139, 121)]\n",
      "clock 84 [(122, 149), (127, 182)]\n",
      "clock 84 [(71, 137), (79, 149)]\n",
      "clock 84 [(127, 148), (135, 182)]\n",
      "[(192, 101), (257, 101), (257, 187), (192, 187)]\n",
      "[[-28   0 -40   0 -40  -2 -28  -2]]\n",
      "Iteration  12\n",
      "[[13.41468457]\n",
      " [13.6318307 ]] (0.47628822695017914, 0.47252486262334475)\n",
      "[[ 0.43950587]\n",
      " [-0.05881041]]\n",
      "Orientation: 0.1234198498451078\n",
      "Depth: 1.1910640186414554\n",
      "couch 62 [(191, 100), (252, 192)]\n",
      "laptop 72 [(48, 50), (136, 116)]\n",
      "clock 84 [(116, 150), (127, 183)]\n",
      "couch 62 [(182, 105), (224, 185)]\n",
      "laptop 72 [(36, 40), (124, 112)]\n",
      "couch 62 [(187, 96), (251, 190)]\n",
      "clock 84 [(112, 147), (121, 181)]\n",
      "[(187, 96), (251, 96), (251, 190), (187, 190)]\n",
      "[[-23   5 -34   5 -34  -5 -23  -5]]\n",
      "Iteration  14\n",
      "[[10.69412884]\n",
      " [10.90145051]] (0.428967946401293, 0.4253748498347456)\n",
      "[[ 0.35092816]\n",
      " [-0.05614962]]\n",
      "Orientation: 0.11768971065889888\n",
      "Depth: 1.1322815307782517\n",
      "couch 62 [(184, 93), (247, 187)]\n",
      "clock 84 [(101, 147), (111, 182)]\n",
      "clock 84 [(111, 146), (120, 181)]\n",
      "clock 84 [(97, 144), (108, 181)]\n",
      "couch 62 [(177, 92), (245, 187)]\n",
      "[(177, 92), (245, 92), (245, 187), (177, 187)]\n",
      "[[-13   9 -28   9 -28  -2 -13  -2]]\n",
      "Iteration  16\n",
      "[[7.371453  ]\n",
      " [7.66508479]] (0.37091235660855315, 0.365454516749483)\n",
      "[[ 0.24434374]\n",
      " [-0.07952527]]\n",
      "Orientation: 0.10341136200868817\n",
      "Depth: 1.067064783303197\n",
      "couch 62 [(175, 91), (238, 188)]\n",
      "clock 84 [(83, 144), (94, 181)]\n",
      "clock 84 [(95, 142), (106, 180)]\n",
      "couch 62 [(174, 89), (238, 187)]\n",
      "couch 62 [(169, 100), (205, 184)]\n",
      "clock 84 [(79, 138), (95, 181)]\n",
      "[(169, 100), (205, 100), (205, 184), (169, 184)]\n",
      "[[-5  1 12  1 12  1 -5  1]]\n",
      "Iteration  18\n",
      "[[-1.14838781]\n",
      " [-1.11346654]] (-0.2644606044541881, -0.26491412746371146)\n",
      "[[-0.03675513]\n",
      " [-0.00945784]]\n",
      "Orientation: 0.08842110640175396\n",
      "Depth: 1.0240611357052711\n",
      "couch 62 [(170, 88), (234, 186)]\n",
      "clock 84 [(66, 140), (79, 182)]\n",
      "couch 62 [(165, 88), (227, 194)]\n",
      "clock 84 [(65, 138), (79, 181)]\n",
      "[(165, 88), (227, 88), (227, 194), (165, 194)]\n",
      "[[ -1  13 -10  13 -10  -9  -1  -9]]\n",
      "Iteration  20\n",
      "[[1.87884136]\n",
      " [1.98800181]] (0.2758182052683315, 0.2744005371535511)\n",
      "[[ 0.0628362 ]\n",
      " [-0.02956429]]\n",
      "Orientation: 0.078296073863453\n",
      "Depth: 1.0137323499085504\n",
      "couch 62 [(164, 90), (229, 189)]\n",
      "clock 84 [(64, 138), (79, 183)]\n",
      "couch 62 [(165, 88), (231, 188)]\n",
      "couch 62 [(147, 116), (179, 184)]\n",
      "[(147, 116), (179, 116), (179, 184), (147, 184)]\n",
      "[[ 17 -15  38 -15  38   1  17   1]]\n",
      "Iteration  22\n",
      "[[-8.93505947]\n",
      " [-9.36332805]] (-0.4023107113705759, -0.3945178340979816)\n",
      "[[-0.2973488]\n",
      " [ 0.1159894]]\n",
      "Orientation: 0.07037516709745575\n",
      "Depth: 1.0003671401527603\n",
      "couch 62 [(169, 84), (236, 188)]\n",
      "couch 62 [(168, 88), (233, 187)]\n",
      "clock 84 [(66, 139), (78, 182)]\n",
      "[(168, 88), (233, 88), (233, 187), (168, 187)]\n",
      "[[ -4  13 -16  13 -16  -2  -4  -2]]\n",
      "Iteration  24\n",
      "[[3.56750447]\n",
      " [3.86185069]] (0.30022027299222676, 0.29633122687857166)\n",
      "[[ 0.12072702]\n",
      " [-0.07971877]]\n",
      "Orientation: 0.07220306093542117\n",
      "Depth: 1.0152877184360418\n",
      "couch 62 [(167, 89), (231, 189)]\n",
      "clock 84 [(63, 138), (79, 183)]\n",
      "couch 62 [(170, 84), (235, 189)]\n",
      "[(170, 84), (235, 84), (235, 189), (170, 189)]\n",
      "[[ -6  17 -18  17 -18  -4  -6  -4]]\n",
      "Iteration  26\n",
      "[[4.19717656]\n",
      " [4.52311081]] (0.3125113533736988, 0.3064530958211861)\n",
      "[[ 0.14170467]\n",
      " [-0.08827386]]\n",
      "Orientation: 0.058436590478566454\n",
      "Depth: 0.9933204072764531\n",
      "couch 62 [(167, 83), (235, 189)]\n",
      "couch 62 [(171, 79), (241, 189)]\n",
      "[(171, 79), (241, 79), (241, 189), (171, 189)]\n",
      "[[ -7  22 -24  22 -24  -4  -7  -4]]\n",
      "Iteration  28\n",
      "[[5.35148497]\n",
      " [5.76275111]] (0.335552994578287, 0.32790864250454266)\n",
      "[[ 0.18060634]\n",
      " [-0.11138458]]\n",
      "Orientation: 0.040709217595914865\n",
      "Depth: 0.966693188922037\n",
      "couch 62 [(172, 77), (247, 191)]\n",
      "clock 84 [(63, 137), (79, 184)]\n",
      "FPS: 3.4\n"
     ]
    }
   ],
   "source": [
    "# Display camera image with bounding boxes for detected objects\n",
    "display(widgets.HBox([image_widget]))\n",
    "\n",
    "# robot pose - may need to be updated\n",
    "depth = 1.3\n",
    "orientation = 0 \n",
    "\n",
    "# Control loop parameters\n",
    "gain = 1.5\n",
    "Rtime = 0.1  # The loop run at 8fps --> 0.125s duration\n",
    "num_iter = 30\n",
    "interval = 2\n",
    "\n",
    "no_motion = False  # Flag to disable motor (for debugging)\n",
    "debug = True\n",
    "\n",
    "# Use stool as reference object\n",
    "ref_obj = 62\n",
    "goal_features = create_feature_pts(end_bbox_stool)\n",
    "print(ref_obj,COCO_labels[ref_obj], goal_features)\n",
    "\n",
    "start = time.perf_counter()\n",
    "# Run for fixed # of iterations\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    image = undistort(camera.value, mtx, dist) # undistort camera image\n",
    "    detections = model(image) # Use SSD model to detect objects\n",
    "    display_detected(image, detections, width, height, debug=True) # put bounding boxes on detected objects\n",
    "    \n",
    "    # Detect reference object in camera image and place green bounding box around it\n",
    "    obj_features = generate_ref_obj_features(ref_obj, image, detections, width, height)\n",
    "    \n",
    "    if obj_features is None:\n",
    "        print(\"Reference object not detected!!!\")\n",
    "        continue\n",
    "        \n",
    "    # bound reference object's feature points at goal pose in red\n",
    "    cv2.rectangle(image, goal_features[0], goal_features[2], RED, 2)\n",
    "    error = features_error(obj_features,goal_features)\n",
    "    \n",
    "    if i%interval == 0 and debug:\n",
    "        print(obj_features)\n",
    "        print(error)\n",
    "    \n",
    "    if np.linalg.norm(error) < 5.6:\n",
    "        print (\"Goal point achieved!!!\")\n",
    "        continue\n",
    "        \n",
    "    T = control2robot(wheel_radius, axle_length) # wheel speeds to robot velocities transform\n",
    "    H = robot2world(orientation)  # robot velocities to world frame transform\n",
    "    \n",
    "    # Image Jacobian based on 4 fp of reference object\n",
    "    UL_fp, UR_fp, LR_fp, LL_fp = obj_features\n",
    "    L = np.vstack((image_jacobian(UL_fp, mtx, depth), image_jacobian(UR_fp, mtx, depth), \\\n",
    "              image_jacobian(LR_fp, mtx, depth), image_jacobian(LL_fp, mtx, depth)))\n",
    "    \n",
    "    # implement w = gain * pinv(LHT).error\n",
    "    J = np.dot(L,np.dot(H,T))\n",
    "    inv_J = np.linalg.pinv(J)\n",
    "    wheel_velocities = gain * np.dot(np.linalg.pinv(J), error.T)\n",
    "    robot_velocities = np.dot(T, wheel_velocities)\n",
    "    w_r = omega2speed(wheel_velocities[0,0],wheel_calibration) \n",
    "    w_l = omega2speed(wheel_velocities[1,0],wheel_calibration) \n",
    "    \n",
    "    if i%interval == 0 and debug:\n",
    "        print(\"Iteration \", i)\n",
    "        save_snapshot(image, diag_dir, i)\n",
    "        print(wheel_velocities, (w_l, w_r))\n",
    "        print(robot_velocities)\n",
    "        print(\"Orientation:\", orientation)\n",
    "        print(\"Depth:\", depth)\n",
    "    \n",
    "    if no_motion is False:\n",
    "        robot.set_motors(w_l, w_r)\n",
    "        time.sleep(Rtime)\n",
    "        robot.stop()\n",
    "    \n",
    "    # Update robot's orientation\n",
    "    translation_velocity = robot_velocities[0,0]\n",
    "    angular_velocity = robot_velocities[1,0]\n",
    "    orientation += Rtime*angular_velocity\n",
    "    depth -= Rtime*translation_velocity*math.cos(orientation)\n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(image)  # update image widget with camera image\n",
    "\n",
    "image_widget.value = bgr8_to_jpeg(image)\n",
    "robot.stop()\n",
    "\n",
    "end = time.perf_counter()\n",
    "print (\"FPS: {:.1f}\".format(num_iter/(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 couch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2a4a7feebd43e48a71089ff142453b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couch  at  [(42, 109), (96, 109), (96, 185), (42, 185)]\n",
      "couch  at  [(61, 109), (109, 109), (109, 186), (61, 186)]\n",
      "couch  at  [(76, 108), (122, 108), (122, 185), (76, 185)]\n",
      "couch  at  [(90, 108), (133, 108), (133, 183), (90, 183)]\n",
      "Reference object near focal center !!!\n"
     ]
    }
   ],
   "source": [
    "# Use chair as reference object\n",
    "ref_obj = 62\n",
    "print(ref_obj,COCO_labels[ref_obj])\n",
    "\n",
    "# Display camera image with bounding boxes for detected objects\n",
    "display(widgets.HBox([image_widget]))\n",
    "\n",
    "Rtime = 0.05\n",
    "\n",
    "for i in range(50):\n",
    "    rotate_left_2wheels(Rtime)\n",
    "    \n",
    "    image = undistort(camera.value, mtx, dist) # undistort camera image\n",
    "    detections = model(image) # Use SSD model to detect objects\n",
    "    display_detected(image, detections, width, height, debug=False) # put bounding boxes on detected objects\n",
    "    \n",
    "    # Detect reference object in camera image and place green bounding box around it\n",
    "    obj_features = generate_ref_obj_features(ref_obj, image, detections, width, height)\n",
    "    obj_center = object_center(obj_features)\n",
    "    \n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(image)  # update image widget with camera image\n",
    "    \n",
    "    if obj_features is None:\n",
    "        print(\"Reference object not detected!!!\")\n",
    "        continue\n",
    "    else:\n",
    "        print(COCO_labels[ref_obj], \" at \", obj_features)\n",
    "        if np.linalg.norm(focal_center - object_center(obj_features)) < 57:\n",
    "            print(\"Reference object near focal center !!!\")\n",
    "            break\n",
    "        continue\n",
    " \n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0160638 ]\n",
      " [-0.25897758]]\n"
     ]
    }
   ],
   "source": [
    "obj_features = [(230, 79), (298, 79), (298, 188), (230, 188)]\n",
    "error = np.array([[-66,  22, -81,  22, -81,  -3, -66,  -3]])\n",
    "\n",
    "orientation = 0.16763945741122677\n",
    "depth = 1.2174696136670464\n",
    "\n",
    "T = control2robot(wheel_radius, axle_length) # wheel speeds to robot velocities transform\n",
    "H = robot2world(orientation)  # robot velocities to world frame transform\n",
    "\n",
    "# Image Jacobian based on 4 fp of reference object\n",
    "UL_fp, UR_fp, LR_fp, LL_fp = obj_features\n",
    "L = np.vstack((image_jacobian(UL_fp, mtx, depth), image_jacobian(UR_fp, mtx, depth), \\\n",
    "              image_jacobian(LR_fp, mtx, depth), image_jacobian(LL_fp, mtx, depth)))\n",
    "    \n",
    "# implement w = gain * pinv(LHT).error\n",
    "J = np.dot(L,np.dot(H,T))\n",
    "inv_J = np.linalg.pinv(J)\n",
    "\n",
    "wheel_velocities = gain * np.dot(np.linalg.pinv(J), error.T)\n",
    "robot_velocities = np.dot(T, wheel_velocities)\n",
    "\n",
    "print (robot_velocities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.54403711386931"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array([f_u,f_v]) - object_center(obj_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([141.95965107, 183.45309821])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([f_u,f_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 46.5, 146. ])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_center(obj_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([95.45965107, 37.45309821])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([f_u,f_v])-object_center(obj_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.568542494923804"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array([40,40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
