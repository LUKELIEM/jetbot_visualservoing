{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Based Visual Servoing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import camera calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "\n",
    "# Load COCO labels\n",
    "filename = \"coco_labels.dat\"\n",
    "filehandler = open(filename, 'rb')\n",
    "COCO_labels = pickle.load(filehandler)\n",
    "\n",
    "# Load camera calibration data for undistort\n",
    "filename = \"calibration.dat\"\n",
    "filehandler = open(filename, 'rb')\n",
    "camera_cal = pickle.load(filehandler)\n",
    "\n",
    "mtx = camera_cal['mtx']\n",
    "dist = camera_cal['dist']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('../Notebooks/object_following/ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up camera for object detection\n",
    "* Camera is set up for 300x300 pixel video\n",
    "* Use undistort() to undistort camera image prior to object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(img, mtx, dist, crop=False):\n",
    "   \n",
    "    h,  w = img.shape[:2]\n",
    "    print (h,w)\n",
    "    \n",
    "    newcameramtx, roi=cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))\n",
    "    \n",
    "    # undistort\n",
    "    dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
    "\n",
    "    # crop the image\n",
    "    if crop:\n",
    "        x,y,w,h = roi\n",
    "        dst = dst[y:y+h, x:x+w]\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85a3744868246f08ad229ced0560d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "display(widgets.HBox([image_widget]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n",
      "72 laptop 0.8606036901473999 [0.2503645420074463, 0.2957329750061035, 0.4554458260536194, 0.4502370357513428]\n"
     ]
    }
   ],
   "source": [
    "undistort_image = undistort(camera.value, mtx, dist)\n",
    "detections = model(undistort_image)\n",
    "\n",
    "items = detections[0]\n",
    "for item in items:\n",
    "    print(item['label'],COCO_labels[item['label']], item['confidence'], item['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop [(75, 88), (136, 135)]\n"
     ]
    }
   ],
   "source": [
    "target = 62\n",
    "\n",
    "# draw all detections on image\n",
    "for det in detections[0]:\n",
    "    label = COCO_labels[det['label']]\n",
    "    bbox = det['bbox']\n",
    "    bbox_pixel = [(int(width * bbox[0]), int(height * bbox[1])), \n",
    "                   (int(width * bbox[2]), int(height * bbox[3]))]\n",
    "    cv2.rectangle(undistort_image, bbox_pixel[0], bbox_pixel[1], (255, 0, 0), 1)\n",
    "    \n",
    "    print(label,bbox_pixel)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == target]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    \n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(undistort_image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 2)\n",
    "\n",
    "image_widget.value = bgr8_to_jpeg(undistort_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Points for Objects\n",
    "\n",
    "From (0,0,0) to (1,0,0): \n",
    "\n",
    "* the TV will act as feature point from Start (0,0,0) to Midpoint (0.5,0,0).\n",
    "* the Stool will act as feature point from Start (0.5,0,0) to Midpoint (1,0,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_bbox_TV = [(75, 88), (136, 135)]\n",
    "midpoint_bbox_TV = [(52, 63), (141, 125)]\n",
    "midpoint_bbox_stool = [(148, 125), (181, 190)]\n",
    "target_bbox_stool = [(153, 105), (202, 187)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_delta(current, desired):\n",
    "    current_x, current_y = current\n",
    "    desired_x, desired_y = desired\n",
    "    return (desired_x-current_x, desired_y-current_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((-23, -26), (4, -10))\n"
     ]
    }
   ],
   "source": [
    "TV_fp1, TV_fp2 = start_bbox_TV\n",
    "desired_TV_fp1, desired_TV_fp2 = midpoint_bbox_TV\n",
    "\n",
    "error_TV = (fp_delta(TV_fp1,desired_TV_fp1), fp_delta(TV_fp2,desired_TV_fp2))\n",
    "print (error_TV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
